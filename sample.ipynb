{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Support Vector Regression\n",
    "\n",
    "Support Vector Regression(SVR) is a type of Support Vector Machine (SVM) that is commonly used for regression tasks, unlike SVM that is commonly used for classification. It was proposed by Vladimir Vapnik, Harris Drucker, Christopher J. C. Burges, Linda Kaufman and Alexander J. Smola in the 1996.\n",
    "\n",
    "In notebook we will see some example in 2D to understand how it works and the difference with SVM but SVR is not limited to 2D data, it can be used for high-dimensional data as well and so in the last part of notebook we will see a real case study with a dataset with n-dimension(n>2).\n",
    "\n",
    "SVR as SVM use Support Vectors that are the data points that are closest to decision boundary(or hyperplane).\n",
    "This methods introduces a loss function which make regression possible, in fact it approximate the output data so that the error is within a certain threshold(ε). ε is added to the model to obtain a sparse solution, so it replace the quadratic loss function(that is common in normalized linear regression) with the ε-insensitive error function. This means that errors within a certain distance ε from the true value are not penalized.\n"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "In the following example we can see how use a SVM for classification, in particular a Linear SVM. It's interesting to go and understand later in the in the next code which is the difference between SVM and SVR.\n",
    "To run the code we use sklearn library."
   ],
   "id": "7329c6fe6c657bf0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "x1 = np.random.randn(50, 2) + np.array([2, 2])\n",
    "x2 = np.random.randn(50, 2) + np.array([6, 6])\n",
    "X = np.vstack((x1, x2))\n",
    "y = np.hstack((np.zeros(50), np.ones(50)))\n",
    "\n",
    "clf = SVC(kernel=\"linear\", C=1e5)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Otteniamo i parametri dell'iperpiano\n",
    "w = clf.coef_[0]\n",
    "b = clf.intercept_[0]\n",
    "\n",
    "# Funzione dell'iperpiano: w0*x + w1*y + b = 0\n",
    "# Risolviamo per y = ...\n",
    "xx = np.linspace(min(X[:,0])-1, max(X[:,0])+1, 100)\n",
    "yy = -(w[0]/w[1]) * xx - b/w[1]\n",
    "\n",
    "# Margini\n",
    "margin = 1 / np.linalg.norm(w)\n",
    "yy_down = yy - np.sqrt(1 + (w[0]/w[1])**2) * margin\n",
    "yy_up   = yy + np.sqrt(1 + (w[0]/w[1])**2) * margin\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.scatter(x1[:,0], x1[:,1], color='blue', label='Class 1')\n",
    "plt.scatter(x2[:,0], x2[:,1], color='red', label='Class 2')\n",
    "\n",
    "# Linea decisionale tratteggiata\n",
    "plt.plot(xx, yy, 'k--', label=\"Decision boundary\")\n",
    "\n",
    "# Margini\n",
    "plt.plot(xx, yy_down, 'k-', linewidth=0.8)\n",
    "plt.plot(xx, yy_up, 'k-', linewidth=0.8)\n",
    "\n",
    "# Support vectors\n",
    "plt.scatter(clf.support_vectors_[:,0], clf.support_vectors_[:,1],\n",
    "            s=100, facecolors='none', edgecolors='k', label=\"Support Vectors\")\n",
    "\n",
    "plt.title(\"Linearly Separable Data with Hyperplane & Margins\")\n",
    "plt.legend()\n",
    "\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the cose above, we generated two sets of 2D data points. The first set is linearly separable, while the second set is not linearly separable. It's a good example to highlight type of data where Linear SVM can work. It's a good example of classification problem where Linear SVM can work.\n",
    "Now the problem is that not always the data are linearly separable so, in that case, We must use other type of SVM. In the follow example we can see an example of non linearly separable data.\n",
    "\n",
    "In the preview code we can see a good example of classification but now with SVR we will see a different process.\n",
    "\n",
    "## SVR\n",
    "\n",
    "### Regression\n",
    "Regression is a statistical and machine learning method used to predict continuous numerical values of a dependent variable from one or more independent variables, by finding a function that best approximates the relationship between inputs and outputs.\n",
    "\n",
    "Below we can see an implementation of SVR with Linear Kernel. This s an easy example to understand how SVR works and what see a regression problem.\n"
   ],
   "id": "7a4d2f1cc03ffed8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = 2 * X.ravel() + 3 + np.random.randn(100) * 2  # relazione lineare con rumore\n",
    "\n",
    "svr = SVR(kernel='linear', epsilon=2)\n",
    "svr.fit(X, y)\n",
    "\n",
    "y_pred = svr.predict(X)\n",
    "\n",
    "epsilon = svr.epsilon\n",
    "y_up = y_pred + epsilon\n",
    "y_down = y_pred - epsilon\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X, y, color='blue', label='Dati')\n",
    "plt.plot(X, y_pred, 'r--', label='Linea centrale (f(x))')\n",
    "plt.plot(X, y_up, 'k-', linewidth=0.8, label='Fascia epsilon')\n",
    "plt.plot(X, y_down, 'k-', linewidth=0.8)\n",
    "plt.fill_between(X.ravel(), y_down, y_up, color='gray', alpha=0.2)\n",
    "plt.legend()\n",
    "plt.title(\"Linear SVR with Epsilon Margin\")\n",
    "plt.show()\n"
   ],
   "id": "6579946c96952080",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It's easy implement SRV with sklearn library. In the example above we generated some 1D data with a linear relationship and some noise. We then fit a Linear SVR model to the data and plotted the results, including the epsilon margin.\n",
    "\n",
    "The parameter $ \\varepsilon $ defines the width of the epsilon-insensitive tube around the regression function.\n",
    "- Small $ \\varepsilon $: the model becomes more sensitive to small deviations from the true values, leading to a tighter fit to the training data.\n",
    "- Large $ \\varepsilon $: the model allows for larger deviations from the true values without penalty, resulting in a smoother and more generalized function."
   ],
   "id": "4141ad2a9e6fbd1b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Mathematical Formulation\n",
    "\n",
    "Linear SVR aims to find a function\n",
    "\n",
    "\\begin{gather*}\n",
    "y_i = w^T \\phi(x_i) + b\n",
    "\\end{gather*}\n",
    "\n",
    "\n",
    "that approximates the data $ (x_i, y_i) $ within a tolerance $ \\varepsilon $.\n",
    "$\\epsilon$-insensibile $E_\\epsilon$ è data da:\n",
    "$$E_{\\epsilon}(y(\\mathbf{x}) - t) = \\begin{cases} 0 & \\text{se } |y(\\mathbf{x}) - t| < \\epsilon \\\\ |y(\\mathbf{x}) - t| - \\epsilon & \\text{altrimenti} \\end{cases} \\quad$$\n",
    "\n",
    "Minimizing model complexity and errors outside the epsilon-insensitive tube:\n",
    "\n",
    "\\begin{gather*}\n",
    "\\min_{w, b, \\xi_i, \\xi_i^*} \\quad \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{m} (\\xi_i + \\hat{\\xi_i})\n",
    "\\end{gather*}\n",
    "\n",
    "With:\n",
    "-  $ \\quad \\frac{1}{2} \\|w\\|^2 $ : complexity of the model (we want to minimize it)\n",
    "-  The second part is for the errors outside the epsilon-intensitive tube.\n",
    "\n",
    "subject to:\n",
    "- For points above the tube:    $ y_i - w^T \\phi(x_i) - b \\leq \\varepsilon + \\xi_i $\n",
    "- For points below the tube:    $ w^T \\phi(x_i) + b - y_i \\leq \\varepsilon + \\hat{\\xi_i} $\n",
    "- Slack variables must be non-negative:     $ \\xi_i, \\hat{\\xi_i} \\geq 0 $\n",
    "\n",
    "Where:\n",
    "- $ w $ are the model weights.\n",
    "- $ b $ is the bias.\n",
    "- $ \\varepsilon $ defines the epsilon-insensitive tube.\n",
    "- $ \\xi_i, \\hat{\\xi_i} $ are slack variables for errors outside the tube.\n",
    "- $ C $ balances model complexity versus tolerance of errors.\n",
    "\n",
    "This formulation can be write in another easy way, without the slack variables.  If the problem is not feasible, we introduce slack variables, which are the data points that fall outside of the ε-insensitive tube.\n",
    "In real cases, most of time the problem is not feasible because it's very strange that all data points fall in the right way.\n",
    "\n",
    "To minimize the objective function we can use the method of Lagrange multipliers, introducing multipliers $ \\alpha_n \\geq 0, \\hat{\\alpha_n} \\geq 0 $ and $ \\mu_n \\geq 0, \\hat{\\mu_n} \\geq 0 $. Now we optimize the Lagrangian:\n",
    "\\begin{gather*}\n",
    "L = \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{m} (\\xi_i + \\hat{\\xi_i}) - \\sum_{i=1}^{m} \\alpha_i (\\varepsilon + \\xi_i + w^T \\phi(x_i) + b - t_i) - \\sum_{i=1}^{m} \\hat{\\alpha_i} (\\varepsilon + \\hat{\\xi_i} - w^T \\phi(x_i) + b + t_i) - \\sum_{i=1}^{m} (\\mu_i \\xi_i + \\hat{\\mu_i} \\hat{\\xi_i})\n",
    "\\end{gather*}\n",
    "\n",
    "I have change $ y_n $ with $ w^T \\phi(x_i) + b $.\n",
    "\n",
    "And now we must find the derivatives and set them to zero:\n",
    "\\begin{gather*}\n",
    "\\frac{\\partial L}{\\partial w} = 0 \\Rightarrow w = \\sum_{i=1}^{m} (\\alpha_i - \\hat{\\alpha_i}) x_i \\\\\n",
    "\\frac{\\partial L}{\\partial b} = 0 \\Rightarrow \\sum_{i=1}^{m} (\\alpha_i - \\hat{\\alpha_i}) = 0 \\\\\n",
    "\\frac{\\partial L}{\\partial \\xi_i} = 0 \\Rightarrow \\alpha_i + \\mu_i = C \\\\\n",
    "\\frac{\\partial L}{\\partial \\hat{\\xi_i}} = 0 \\Rightarrow \\hat{\\alpha_i} + \\hat{\\mu_i} = C\n",
    "\\end{gather*}\n",
    "\n",
    "Now we can substitute the derivatives in the Lagrangian and obtain the dual problem:\n",
    "\\begin{gather*}\n",
    "\\tilde{L}(\\boldsymbol{a}, \\hat{\\boldsymbol{a}}) = -\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} (\\alpha_i - \\hat{\\alpha_i})(\\alpha_j - \\hat{\\alpha_j}) K(x_i, x_j) - \\varepsilon \\sum_{i=1}^{m} (\\alpha_i + \\hat{\\alpha_i}) + \\sum_{i=1}^{m} t_i (\\alpha_i - \\hat{\\alpha_i})\n",
    "\\end{gather*}\n",
    "\n"
   ],
   "id": "8dff3bba7bcbe41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "How can we say we introduce the kernel $ K(x_i, x_j) =  \\phi(x_i)^T \\phi(x_j) $.\n",
    "Now, we have a costrained maximization problem but we can note that we have $a_i \\geq 0$ and $\\hat{a_i} \\geq 0$ and $ \\mu_n \\geq 0, \\hat{\\mu_n} \\geq 0 $ as before. So, thanks to Laplacian derivated we have:\n",
    "\\begin{gather*}\n",
    "0 \\leq \\alpha_i \\leq C \\\\\n",
    "0 \\leq \\hat{\\alpha_i} \\leq C\n",
    "\\end{gather*}\n",
    "\n",
    "And I can put all in the first function that it's write in this chatper to obtain:\n",
    "\\begin{gather*}\n",
    "y(\\boldsymbol{x}) = \\sum_{i=1}^{m} (\\alpha_i - \\hat{\\alpha_i}) K(x_i, x) + b\n",
    "\\end{gather*}\n",
    "\n",
    "with kernel function.\n",
    "The corrisponding Karush-Kuhn-Tucker (KKT) conditions(that are a generalization of the method of Lagrange multipliers and thay are useful for solving optimization problems with constraints), are given by:\n",
    "\\begin{gather*}\n",
    "\\alpha_i (\\varepsilon + \\xi_i + w^T \\phi(x_i) + b - t_i) = 0 \\\\\n",
    "\\hat{\\alpha_i} (\\varepsilon + \\hat{\\xi_i} - w^T \\phi(x_i) + b + t_i) = 0 \\\\\n",
    "(C - a_n)\\xi_i = 0 \\\\\n",
    "(C - \\hat{a_n})\\hat{\\xi_i} = 0 \\\\\n",
    "\\end{gather*}\n",
    "\n",
    "From that we can see that:\n",
    "- If $ 0 < \\alpha_i < C $, then $ y_i - f(x_i) = \\varepsilon $\n",
    "- If $ 0 < \\hat{\\alpha_i} < C $, then $ f(x_i) - y_i = \\varepsilon $\n",
    "- If $ \\alpha_i = 0 $, then $ y_i - f(x_i) < \\varepsilon $\n",
    "- If $ \\hat{\\alpha_i} = 0 $, then $ f(x_i) - y_i < \\varepsilon $\n",
    "- If $ \\alpha_i = C $, then $ y_i - f(x_i) > \\varepsilon $\n",
    "- If $ \\hat{\\alpha_i} = C $, then $ f(x_i) - y_i > \\varepsilon $\n",
    "- If $ \\xi_i > 0 $, then $ \\alpha_i = C $\n",
    "- If $ \\hat{\\xi_i} > 0 $, then $ \\hat{\\alpha_i} = C $\n",
    "- If $ \\xi_i = 0 $, then $ \\alpha_i < C $\n",
    "- If $ \\hat{\\xi_i} = 0 $, then $ \\hat{\\alpha_i} < C $\n",
    "- If $ |y_i - f(x_i)| < \\varepsilon $, then $ \\alpha_i = 0 $ and $ \\hat{\\alpha_i} = 0 $\n",
    "- If $ |y_i - f(x_i)| = \\varepsilon $, then $ 0 < \\alpha_i < C $ or $ 0 < \\hat{\\alpha_i} < C $\n",
    "- If $ |y_i - f(x_i)| > \\varepsilon $, then $ \\alpha_i = C $ or $ \\hat{\\alpha_i} = C $\n",
    "\n",
    "Where $ f(x_i) = w^T \\phi(x_i) + b $.\n"
   ],
   "id": "be3ee67fefdab214"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Non-linear SVR with polynomial kernel\n",
    "Now we can try to use a non-linear kernel, in particular a polynomial kernel. We can see how the model change with respect to the linear kernel. We can choose the degree of the polynomial kernel but carefully to avoid overfitting.\n"
   ],
   "id": "22471e375cbda8f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Dati di esempio (1D per semplicità)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = 0.4 * (X.ravel() - 5)**3 + 2 * X.ravel() + np.random.randn(100) * 2  # relazione non lineare\n",
    "\n",
    "# Modello SVR con kernel polinomiale\n",
    "svr_poly = SVR(kernel='poly', degree=3, C=100, epsilon=0.01, coef0=10)\n",
    "svr_poly.fit(X, y)\n",
    "\n",
    "# Predizione\n",
    "y_pred = svr_poly.predict(X)\n",
    "\n",
    "# Fascia epsilon\n",
    "epsilon = svr_poly.epsilon\n",
    "y_up = y_pred + epsilon\n",
    "y_down = y_pred - epsilon\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X, y, color='blue', label='Data')\n",
    "plt.plot(X, y_pred, 'r--', label='Central function f(x)')\n",
    "plt.plot(X, y_up, 'k-', linewidth=0.8, label='Epsilon margin')\n",
    "plt.plot(X, y_down, 'k-', linewidth=0.8)\n",
    "plt.fill_between(X.ravel(), y_down, y_up, color='gray', alpha=0.2)\n",
    "plt.legend()\n",
    "plt.title(\"Polynomial SVR with Epsilon Margin\")\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "495ceb547816aa69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As above, you can change some parameters and see how they affect the model, in particular the degree of the polynomial kernel.\n",
    "$ coef0 $ shifts the polynomial kernel, allowing lower-degree terms to influence the model and making it more flexible. If your data includes negative values, as in the plot, setting a positive $ coef0 $ helps the model capture them instead of excluding them.\n",
    "If you want you can try to change some parameters like $ C $ and epsilon to see how they affect the model.\n",
    "The parameter $ C $ in an SVR controls the trade-off between model complexity and training error.\n",
    "- Small $ C $: the model becomes “softer,” meaning it allows more errors to keep a larger margin or simpler function. This can help prevent overfitting but may increase training error.\n",
    "- Large $ C $: the model tries to minimize training errors, even if this means a narrower margin or more complex function. This can improve accuracy on the training set but increases the risk of overfitting on new data.\n",
    "\n",
    "This is the formula: $$\n",
    "K(x, x') = (\\gamma \\langle x, x' \\rangle + \\text{coef0})^{d}\n",
    "$$\n",
    "\n",
    "There are other type of kernel that We can try like RBF kernel or Sigmoid kernel.\n",
    "\n",
    "## RBF Kernel\n",
    "\n",
    "The RBF kernel (Radial Basis Function) measures similarity between points based on their distance from each other. “Radial” means the function depends only on the distance from a center and “basis function” refers to using these localized bumps to build the model. The parameter gamma controls how quickly similarity decreases: large gamma makes the model focus on nearby points (more complex), small gamma makes it smoother and more global.\n",
    "If gamma is too large, the model may overfit the training data, capturing noise instead of the underlying pattern. If gamma is too small, the model may underfit, failing to capture important patterns in the data.\n"
   ],
   "id": "27294e13e3edc086"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dati di esempio (1D per semplicità)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = 0.4 * (X.ravel() - 5)**3 + 2 * X.ravel() + np.random.randn(100) * 2  # relazione non lineare\n",
    "\n",
    "# Modello SVR con kernel RBF\n",
    "svr_rbf = SVR(kernel='rbf', C=100, epsilon=1, gamma=0.5)\n",
    "svr_rbf.fit(X, y)\n",
    "\n",
    "# Predizione\n",
    "y_pred = svr_rbf.predict(X)\n",
    "\n",
    "# Fascia epsilon\n",
    "epsilon = svr_rbf.epsilon\n",
    "y_up = y_pred + epsilon\n",
    "y_down = y_pred - epsilon\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X, y, color='blue', label='Data')\n",
    "plt.plot(X, y_pred, 'r--', label='Central function f(x)')\n",
    "plt.plot(X, y_up, 'k-', linewidth=0.8, label='Epsilon margin')\n",
    "plt.plot(X, y_down, 'k-', linewidth=0.8)\n",
    "plt.fill_between(X.ravel(), y_down, y_up, color='gray', alpha=0.2)\n",
    "plt.legend()\n",
    "plt.title(\"RBF SVR with Epsilon Margin\")\n",
    "plt.show()\n"
   ],
   "id": "aa68237947d90cb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "# Dati di esempio (1D per semplicità)\n",
    "X = np.linspace(-10, 10, 100).reshape(-1, 1)  # range più ampio per la sigmoide\n",
    "y_true = 1 / (1 + np.exp(-X.ravel()))         # funzione sigmoide\n",
    "y = y_true + np.random.randn(100) * 0.05\n",
    "\n",
    "# Modello SVR con kernel Sigmoid\n",
    "svr_sigmoid = SVR(kernel='sigmoid', C=100, epsilon=0.4, gamma=0.7, coef0=6)\n",
    "svr_sigmoid.fit(X, y)\n",
    "\n",
    "# Predizione\n",
    "y_pred = svr_sigmoid.predict(X)\n",
    "\n",
    "# Fascia epsilon\n",
    "epsilon = svr_sigmoid.epsilon\n",
    "y_up = y_pred + epsilon\n",
    "y_down = y_pred - epsilon\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X, y, color='blue', label='Data')\n",
    "plt.plot(X, y_pred, 'r--', label='Central function f(x)')\n",
    "plt.plot(X, y_up, 'k-', linewidth=0.8, label='Epsilon margin')\n",
    "plt.plot(X, y_down, 'k-', linewidth=0.8)\n",
    "plt.fill_between(X.ravel(), y_down, y_up, color='gray', alpha=0.2)\n",
    "plt.legend()\n",
    "plt.title(\"Sigmoid SVR with Epsilon Margin\")\n",
    "plt.show()\n"
   ],
   "id": "bc171672a480733b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sigmoid Kernel\n",
   "id": "7d73d1f3dfdc9589"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Real Case Study\n",
    "Now we can see a real case study with a dataset. We can try with different type of kernel over dataset.\n",
    "The dataset that we use are:\n",
    "- Metro_Interstate_traffic_Volume.csv: It have 9 columns and 48204 rows. The target is traffic_volume.\n",
    "- Used_Car_Price_Prediction.csv: It have 10 columns and 1000 rows. The target is sale_price.\n",
    "- Fish.csv: It have 7 columns and 159 rows. The target is Weight."
   ],
   "id": "5972c87d0116c29a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Caricamento dataset\n",
    "df = pd.read_csv('./data/Used_Car_Price_Prediction.csv')\n",
    "data = df['sale_price'].dropna().values   # tolgo eventuali NaN\n",
    "data = np.sort(data).reshape(-1, 1)       # colonna ordinata\n",
    "\n",
    "# Creazione asse x (indici)\n",
    "x = np.arange(len(data)).reshape(-1, 1)\n",
    "\n",
    "# Normalizzazione\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "x_scaled = scaler_x.fit_transform(x)\n",
    "y_scaled = scaler_y.fit_transform(data)\n",
    "\n",
    "# Modello SVR\n",
    "svr_poly = SVR(kernel='poly', degree=4, C=100, epsilon=0.05, coef0=1)\n",
    "svr_poly.fit(x_scaled, y_scaled.ravel())\n",
    "\n",
    "# Predizione (su scala normalizzata)\n",
    "y_pred_scaled = svr_poly.predict(x_scaled)\n",
    "\n",
    "# Ritorno alla scala originale\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
    "y_up = scaler_y.inverse_transform((y_pred_scaled + svr_poly.epsilon).reshape(-1, 1))\n",
    "y_down = scaler_y.inverse_transform((y_pred_scaled - svr_poly.epsilon).reshape(-1, 1))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x, data, color='blue', s=10, alpha=0.6, label='Data')\n",
    "plt.plot(x, y_pred, 'r--', linewidth=2, label='SVR Prediction')\n",
    "plt.plot(x, y_up, 'k-', linewidth=0.8, label='Epsilon margin')\n",
    "plt.plot(x, y_down, 'k-', linewidth=0.8)\n",
    "plt.fill_between(x.ravel(), y_down.ravel(), y_up.ravel(), color='gray', alpha=0.2)\n",
    "plt.legend()\n",
    "plt.title(\"Polynomial SVR with Normalized Data\")\n",
    "plt.xlabel(\"Index (sorted cars)\")\n",
    "plt.ylabel(\"Sale Price\")\n",
    "plt.show()\n"
   ],
   "id": "619cff32d10e3863",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "936cf762d763fe68",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
